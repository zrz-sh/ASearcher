{
    "instance_id": "ws_en_005",
    "score": 0.0,
    "precision_by_row": 0.3125,
    "recall_by_row": 0.3125,
    "f1_by_row": 0.3125,
    "precision_by_item": 0.78125,
    "recall_by_item": 0.78125,
    "f1_by_item": 0.78125,
    "msg": "    country_llm_judge  university_exact_match  alliance_llm_judge  minimumgparequirement_llm_judge\n0                   1                     1.0                   1                                1\n1                   1                     1.0                   1                                0\n2                   1                     1.0                   1                                0\n3                   1                     1.0                   1                                1\n4                   1                     1.0                   1                                1\n5                   1                     1.0                   1                                1\n6                   1                     1.0                   1                                1\n7                   1                     1.0                   1                                0\n8                   1                     1.0                   1                                0\n9                   1                     1.0                   1                                0\n10                  1                     1.0                   1                                0\n11                  1                     1.0                   1                                0\n12                  1                     1.0                   1                                0\n13                  1                     1.0                   1                                0\n14                  1                     1.0                   1                                0"
}