{
    "instance_id": "ws_en_005",
    "score": 0.0,
    "precision_by_row": 0.058823529411764705,
    "recall_by_row": 0.0625,
    "f1_by_row": 0.06060606060606061,
    "precision_by_item": 0.6764705882352942,
    "recall_by_item": 0.71875,
    "f1_by_item": 0.696969696969697,
    "msg": "    country_llm_judge  university_exact_match  alliance_llm_judge  minimumgparequirement_llm_judge\n0                   1                     1.0                   1                                0\n1                   1                     1.0                   1                                1\n2                   1                     1.0                   1                                0\n3                   1                     1.0                   1                                0\n4                   1                     1.0                   1                                0\n5                   1                     1.0                   1                                0\n6                   1                     1.0                   1                                0\n7                   1                     1.0                   1                                0\n8                   1                     1.0                   1                                0\n9                   1                     1.0                   1                                0\n10                  1                     1.0                   1                                0\n11                  1                     1.0                   1                                0\n12                  1                     1.0                   1                                0\n13                  1                     1.0                   1                                0\n14                  1                     1.0                   1                                0"
}